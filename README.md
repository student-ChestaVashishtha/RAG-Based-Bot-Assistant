# ğŸ“„ PDF Intel-Link

**Faithful Question Answering over Documents using Retrieval-Augmented Generation**

## Overview

PDF Intel-Link is an applied AI system designed to enable **faithful, traceable question answering over unstructured PDF documents**. The system follows a **retrieval-first, hallucination-aware architecture**, ensuring that all generated answers are strictly grounded in the source document.

The project explores the intersection of **Natural Language Understanding, Information Retrieval, and Large Language Models**, with a strong emphasis on **experimental rigor, system reliability, and real-world usability**â€”key principles aligned with Microsoft Researchâ€™s applied AI philosophy.

---

## ğŸš€ Features

* ğŸ” **Context-aware question answering** over PDFs
* ğŸ§  **RAG pipeline** with persistent vector storage (ChromaDB)
* ğŸ“‘ **Page-number grounded answers** to improve trust and traceability
* âŒ **Anti-hallucination prompt design** (answers strictly from document context)
* ğŸ’¾ **Persistent embeddings** for faster repeated queries
* ğŸ›ï¸ **Simple Gradio web interface**
* âš¡ Powered by **Google Gemini** and **MiniLM embeddings**

---

## Motivation

Large Language Models are prone to hallucination when answering questions over long documents. This project investigates how **retrieval-augmented pipelines, prompt constraints, and citation-aware context formatting** can significantly improve answer faithfulness while maintaining usability.

---

## Research Contributions

This project makes the following applied research contributions:

### 1. **Hallucination Mitigation via Prompt-Constrained RAG**

* Designed a **strict prompt protocol** that enforces:

  * Context-only answering
  * Explicit refusal when evidence is absent
* Evaluated zero-temperature inference to reduce generative variance.

**Insight:** Prompt-level constraints combined with retrieval significantly reduce unsupported responses compared to vanilla LLM querying.

---

### 2. **Citation-Aware Context Formatting**

* Introduced **page-numberâ€“annotated context injection** during retrieval.
* Enabled traceable answers that map directly back to the original document.

**Insight:** Lightweight metadata grounding improves user trust and answer verifiability without additional model complexity.

---

### 3. **Persistent Vector Store for Repeated Query Efficiency**

* Implemented a **persistent ChromaDB-backed embedding store**, enabling reuse across sessions.
* Reduced redundant embedding computation for large documents.

**Insight:** Persistence improves system efficiency and mirrors production-scale document intelligence systems.

---

### 4. **End-to-End RAG System using LCEL**

* Built the pipeline using **LangChain Expression Language (LCEL)** for modularity and composability.
* Supports clean separation between retrieval, prompting, and generation.

**Insight:** LCEL-based design simplifies experimentation and rapid system iteration.

---

## System Architecture

1. **Document Ingestion**

   * PDF parsing using `PyPDFLoader`
2. **Text Segmentation**

   * Overlapping chunking to preserve semantic continuity
3. **Embedding Generation**

   * Sentence-level semantic embeddings via MiniLM
4. **Vector Indexing**

   * Persistent ChromaDB storage
5. **Context Retrieval**

   * Top-k semantic similarity search
6. **Constrained Generation**

   * Gemini LLM with anti-hallucination prompt
7. **Interactive Interface**

   * Gradio-based UI for rapid experimentation

---

## Technical Stack

* **Language:** Python
* **LLM:** Google Gemini (gemini-2.5-flash)
* **Retrieval Framework:** LangChain (LCEL)
* **Embeddings:** HuggingFace `all-MiniLM-L6-v2`
* **Vector Database:** ChromaDB (persistent)
* **UI:** Gradio

---

## Experimental Design Choices

* Zero temperature decoding for determinism
* Chunk overlap to prevent boundary information loss
* Retrieval-first architecture (no free-form LLM answers)
* Explicit fallback response for unanswerable queries

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/pdf-intel-link.git
cd pdf-intel-link
pip install -r requirements.txt
```

---

## ğŸ”‘ Prerequisites

* Python 3.9+
* Google Gemini API Key

  * Get it from: [https://ai.google.dev/](https://ai.google.dev/)

  ---

  ---

## â–¶ï¸ Usage

```bash
python app.py
```

1. Launches a local Gradio app
2. Enter your **Gemini API key**
3. Upload a **PDF document**
4. Ask questions grounded in the document

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ chroma_db/          # Persistent vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---
  
## Applications

* Research paper analysis
* Academic and technical document comprehension
* Policy and compliance document querying
* AI-assisted reading with verifiable citations

---

## Limitations & Future Work

* Quantitative faithfulness evaluation (e.g., attribution metrics)
* Hybrid retrieval (BM25 + dense embeddings)
* Cross-document reasoning
* Query re-ranking and reretrieval
* Comparative analysis across LLMs

---

## Author

**Chesta Vashishtha**
B.Tech Undergraduate | Applied Machine Learning & NLP
Interested in Applied AI Research, LLM Systems, and Information Retrieval

---










